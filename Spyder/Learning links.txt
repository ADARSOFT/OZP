Other data resources
https://www.kaggle.com/janiobachmann/bank-marketing-campaign-opening-a-term-deposit
https://www.kaggle.com/aleksandradeis/bank-marketing-analysis
https://www.kaggle.com/goldens/bank-marketing-eda-model-and-plotly

Data preparation and exploration

https://www.analyticsvidhya.com/blog/2015/09/build-predictive-model-10-minutes-python/
https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html
https://www.kdnuggets.com/2017/06/7-steps-mastering-data-preparation-python.html

Prepare data for predictive modeling
https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/


Missing values:

https://clevertap.com/blog/how-to-treat-missing-values-in-your-data-part-i/
https://clevertap.com/blog/how-to-treat-missing-values-in-your-data-part-ii/
https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html

https://stats.stackexchange.com/questions/225175/why-do-some-people-use-999-or-9999-to-replace-missing-values

All use case resolved "towards data science"
https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

Feature selection: (pandas)
https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b
https://scikit-learn.org/stable/modules/feature_selection.html

One Hot Encoding (get_dummies) vs Label encoding:
https://towardsdatascience.com/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b

Finding outliers:

Boxplot (interquartile technique)
https://datascience.stackexchange.com/questions/54808/how-to-remove-outliers-using-box-plot

https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba

When should drop outliers 
https://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/
https://humansofdata.atlan.com/2018/03/when-delete-outliers-dataset/

PCA
https://towardsdatascience.com/why-feature-correlation-matters-a-lot-847e8ba439c4
https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe
https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60
https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html


PIPELINE 
https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html
https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976
https://scikit-learn.org/stable/modules/compose.html
https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html
https://scikit-learn.org/stable/modules/compose.html#combining-estimators
	
Clustering KMEANS
https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient

Feature selection 
https://scikit-learn.org/stable/modules/feature_selection.html

Imbalanced data
https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18

Ensemble algorithms

XGBoost: How it works, with an example.
https://www.youtube.com/watch?v=OQKQHNCVf5k

Jaroslaw Szymczak - Gradient Boosting in Practice: a deep dive into xgboost
https://www.youtube.com/watch?v=s3VmuVPfu0s

https://en.wikipedia.org/wiki/Ensemble_learning#Stacking
https://blog.statsbot.co/ensemble-learning-d1dcd548e936
https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725
https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205?gi=89b0ec6fff5e
https://www.toptal.com/machine-learning/ensemble-methods-machine-learning
https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/
https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions
https://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html